{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.18.0\n"
     ]
    }
   ],
   "source": [
    "# import library yang dibutuhkan\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import shutil\n",
    "import zipfile,os\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing import image as keras_image\n",
    "import matplotlib.pyplot as plt\n",
    "import cv2\n",
    "from rembg.bg import remove\n",
    "import easygui\n",
    "from PIL import Image\n",
    "from concurrent.futures import ProcessPoolExecutor\n",
    "import wget\n",
    "\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Direktori sudah ada isinya\n"
     ]
    }
   ],
   "source": [
    "# check directory train ada isinya atau ga\n",
    "if os.path.exists('./train'):\n",
    "    print('Direktori sudah ada isinya')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from rembg import remove\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "# fungsi untuk menghapus background dari sebuah gambar\n",
    "def remove_background(input_path, output_path):\n",
    "    try:\n",
    "        input_image = Image.open(input_path) # ngebuka filenya\n",
    "        output_image = remove(input_image) # ngehapus background -> library rembg\n",
    "        output_image.save(output_path) # nyimpen yang udah diapus ke background\n",
    "    except Exception as e: # kalo gagal nyimpen yg removed background disimpen di log\n",
    "        with open(\"error_log.txt\", \"a\") as log_file:\n",
    "            log_file.write(f\"Error processing {input_path}: {e}\\n\")\n",
    "\n",
    "\n",
    "# # fungsi untuk memproses penghapusan background secara rekursif\n",
    "# def remove_background_serial(input_dir, output_dir):\n",
    "#     for root, dirs, files in os.walk(input_dir):  # Rekursif ke subfolder\n",
    "        \n",
    "#         # root : directory saat ini yang sedang diproses\n",
    "#         # dirs : subdirectory dalam root\n",
    "#         # files : file dalam root\n",
    "        \n",
    "#         # bikin biar strukturnya masih sama\n",
    "#         relative_path = os.path.relpath(root, input_dir)\n",
    "#         current_output_dir = os.path.join(output_dir, relative_path)\n",
    "\n",
    "#         # root harus ada, kalau gaada bikin dulu\n",
    "#         if not os.path.exists(current_output_dir):\n",
    "#             os.makedirs(current_output_dir)\n",
    "\n",
    "#         # loop setiap directory\n",
    "#         for filename in files:\n",
    "#             input_path = os.path.join(root, filename)\n",
    "#             output_path = os.path.join(current_output_dir, filename)\n",
    "\n",
    "#             remove_background(input_path, output_path)\n",
    "#             print(f\"Processed: {input_path} -> {output_path}\")\n",
    "\n",
    "def remove_background_serial(input_dir, output_dir):\n",
    "    for root, dirs, files in os.walk(input_dir):\n",
    "        relative_path = os.path.relpath(root, input_dir)\n",
    "        current_output_dir = os.path.join(output_dir, relative_path)\n",
    "\n",
    "        if not os.path.exists(current_output_dir):\n",
    "            os.makedirs(current_output_dir)\n",
    "\n",
    "        for filename in files:\n",
    "            input_path = os.path.join(root, filename)\n",
    "            output_path = os.path.join(current_output_dir, filename)\n",
    "\n",
    "            try:\n",
    "                # Cek apakah input file adalah gambar\n",
    "                if not filename.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "                    print(f\"Skipped non-image file: {filename}\")\n",
    "                    continue\n",
    "\n",
    "                # Proses penghapusan background\n",
    "                remove_background(input_path, output_path)\n",
    "                print(f\"Processed: {input_path} -> {output_path}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                # Log error jika gagal\n",
    "                print(f\"Error processing {input_path}: {e}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed: ./train/subject-2/fall/2_forward_falls/frame024.jpg -> ./processed/subject-2/fall/2_forward_falls/frame024.jpg\n",
      "Processed: ./train/subject-2/fall/2_forward_falls/frame030.jpg -> ./processed/subject-2/fall/2_forward_falls/frame030.jpg\n",
      "Processed: ./train/subject-2/fall/2_forward_falls/frame018.jpg -> ./processed/subject-2/fall/2_forward_falls/frame018.jpg\n",
      "Processed: ./train/subject-2/fall/2_forward_falls/frame019.jpg -> ./processed/subject-2/fall/2_forward_falls/frame019.jpg\n",
      "Processed: ./train/subject-2/fall/2_forward_falls/frame031.jpg -> ./processed/subject-2/fall/2_forward_falls/frame031.jpg\n",
      "Processed: ./train/subject-2/fall/2_forward_falls/frame025.jpg -> ./processed/subject-2/fall/2_forward_falls/frame025.jpg\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 25\u001b[0m\n\u001b[1;32m     22\u001b[0m output_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(processed_dataset_dir, subject, category)\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(input_dir):\n\u001b[0;32m---> 25\u001b[0m     \u001b[43mremove_background_serial\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[19], line 61\u001b[0m, in \u001b[0;36mremove_background_serial\u001b[0;34m(input_dir, output_dir)\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;66;03m# Proses penghapusan background\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m     \u001b[43mremove_background\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     62\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessed: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m -> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# Log error jika gagal\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[19], line 11\u001b[0m, in \u001b[0;36mremove_background\u001b[0;34m(input_path, output_path)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     10\u001b[0m     input_image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mopen(input_path) \u001b[38;5;66;03m# ngebuka filenya\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m     output_image \u001b[38;5;241m=\u001b[39m \u001b[43mremove\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_image\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# ngehapus background -> library rembg\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     output_image\u001b[38;5;241m.\u001b[39msave(output_path) \u001b[38;5;66;03m# nyimpen yang udah diapus ke background\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e: \u001b[38;5;66;03m# kalo gagal nyimpen yg removed background disimpen di log\u001b[39;00m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/rembg/bg.py:264\u001b[0m, in \u001b[0;36mremove\u001b[0;34m(data, alpha_matting, alpha_matting_foreground_threshold, alpha_matting_background_threshold, alpha_matting_erode_size, session, only_mask, post_process_mask, bgcolor, force_return_bytes, *args, **kwargs)\u001b[0m\n\u001b[1;32m    261\u001b[0m img \u001b[38;5;241m=\u001b[39m fix_image_orientation(img)\n\u001b[1;32m    263\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m session \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 264\u001b[0m     session \u001b[38;5;241m=\u001b[39m \u001b[43mnew_session\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mu2net\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    266\u001b[0m masks \u001b[38;5;241m=\u001b[39m session\u001b[38;5;241m.\u001b[39mpredict(img, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m    267\u001b[0m cutouts \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/rembg/session_factory.py:44\u001b[0m, in \u001b[0;36mnew_session\u001b[0;34m(model_name, providers, *args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m     sess_opts\u001b[38;5;241m.\u001b[39minter_op_num_threads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOMP_NUM_THREADS\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     42\u001b[0m     sess_opts\u001b[38;5;241m.\u001b[39mintra_op_num_threads \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOMP_NUM_THREADS\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m---> 44\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43msession_class\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msess_opts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproviders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/rembg/sessions/base.py:34\u001b[0m, in \u001b[0;36mBaseSession.__init__\u001b[0;34m(self, model_name, sess_opts, providers, *args, **kwargs)\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     32\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproviders\u001b[38;5;241m.\u001b[39mextend(_providers)\n\u001b[0;32m---> 34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minner_session \u001b[38;5;241m=\u001b[39m \u001b[43mort\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mInferenceSession\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__class__\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_models\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[43mproviders\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mproviders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43msess_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msess_opts\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:465\u001b[0m, in \u001b[0;36mInferenceSession.__init__\u001b[0;34m(self, path_or_bytes, sess_options, providers, provider_options, **kwargs)\u001b[0m\n\u001b[1;32m    462\u001b[0m disabled_optimizers \u001b[38;5;241m=\u001b[39m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdisabled_optimizers\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    464\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 465\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_create_inference_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproviders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprovider_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisabled_optimizers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    466\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mValueError\u001b[39;00m, \u001b[38;5;167;01mRuntimeError\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    467\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_enable_fallback:\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/onnxruntime/capi/onnxruntime_inference_collection.py:537\u001b[0m, in \u001b[0;36mInferenceSession._create_inference_session\u001b[0;34m(self, providers, provider_options, disabled_optimizers)\u001b[0m\n\u001b[1;32m    534\u001b[0m     disabled_optimizers \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m(disabled_optimizers)\n\u001b[1;32m    536\u001b[0m \u001b[38;5;66;03m# initialize the C++ InferenceSession\u001b[39;00m\n\u001b[0;32m--> 537\u001b[0m \u001b[43msess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minitialize_session\u001b[49m\u001b[43m(\u001b[49m\u001b[43mproviders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprovider_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdisabled_optimizers\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    539\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sess \u001b[38;5;241m=\u001b[39m sess\n\u001b[1;32m    540\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sess_options \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sess\u001b[38;5;241m.\u001b[39msession_options\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# path dataset asli dan hasil\n",
    "original_dataset_dir = './train'  # dataset asli\n",
    "processed_dataset_dir = './processed'  # dataset setelah background dihapus\n",
    "processed_split_dir = './processed_split'  # direktori hasil pembagian train/validation\n",
    "\n",
    "# kategori dan daftar subject\n",
    "categories = ['fall', 'non_fall']\n",
    "subjects = [f for f in os.listdir(original_dataset_dir) if os.path.isdir(os.path.join(original_dataset_dir, f))]\n",
    "\n",
    "# # ngapus background\n",
    "# for subject in subjects:\n",
    "#     for category in categories:\n",
    "#         input_dir = os.path.join(original_dataset_dir, subject, category)\n",
    "#         output_dir = os.path.join(processed_dataset_dir, subject, category)\n",
    "\n",
    "#         if os.path.exists(input_dir):\n",
    "#             remove_background_serial(input_dir, output_dir)\n",
    "\n",
    "for subject in subjects[:1]:  # Hanya subject pertama\n",
    "    for category in categories:\n",
    "        input_dir = os.path.join(original_dataset_dir, subject, category)\n",
    "        output_dir = os.path.join(processed_dataset_dir, subject, category)\n",
    "\n",
    "        if os.path.exists(input_dir):\n",
    "            remove_background_serial(input_dir, output_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # membuat direktori untuk data training dan data validasi\n",
    "# base_dir = './processed'\n",
    "# train_dir = os.path.join(base_dir, 'train')\n",
    "# validation_dir = os.path.join(base_dir, 'val')\n",
    "\n",
    "# # membuat direktori untuk data training \n",
    "# train_fall_dir = os.path.join(train_dir, 'fall') \n",
    "# train_non_fall_dir = os.path.join(train_dir, 'non_fall')\n",
    "\n",
    "# # membuat direktori untuk data validasi\n",
    "# validation_fall_dir = os.path.join(validation_dir, 'fall')\n",
    "# validation_non_fall_dir = os.path.join(validation_dir, 'non_fall')\n",
    "\n",
    "# # membuat direktori untuk data training dan data validasi\n",
    "# if not os.path.exists(train_dir):\n",
    "#     os.mkdir(train_dir)\n",
    "# if not os.path.exists(validation_dir):\n",
    "#     os.mkdir(validation_dir)\n",
    "\n",
    "# if not os.path.exists(train_fall_dir):\n",
    "#     os.mkdir(train_fall_dir)\n",
    "# if not os.path.exists(train_non_fall_dir):\n",
    "#     os.mkdir(train_non_fall_dir)\n",
    "\n",
    "# if not os.path.exists(validation_fall_dir):\n",
    "#     os.mkdir(validation_fall_dir)\n",
    "# if not os.path.exists(validation_non_fall_dir):\n",
    "#     os.mkdir(validation_non_fall_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check kosong\n",
      "check kosong\n",
      "check kosong\n",
      "check kosong\n",
      "check kosong\n",
      "check kosong\n",
      "check kosong\n",
      "check kosong\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Membagi dataset menjadi train dan validation\n",
    "train_dir = os.path.join(processed_split_dir, 'train')\n",
    "validation_dir = os.path.join(processed_split_dir, 'val')\n",
    "\n",
    "train_fall_dir = os.path.join(train_dir, 'fall')\n",
    "train_non_fall_dir = os.path.join(train_dir, 'non_fall')\n",
    "validation_fall_dir = os.path.join(validation_dir, 'fall')\n",
    "validation_non_fall_dir = os.path.join(validation_dir, 'non_fall')\n",
    "\n",
    "# Membuat semua direktori jika belum ada\n",
    "for directory in [train_fall_dir, train_non_fall_dir, validation_fall_dir, validation_non_fall_dir]:\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "# Step 3: Membagi dataset\n",
    "train_fall = []\n",
    "validation_fall = []\n",
    "train_non_fall = []\n",
    "validation_non_fall = []\n",
    "\n",
    "for subject in subjects:\n",
    "    # Fall\n",
    "    fall_dir = os.path.join(processed_dataset_dir, subject, 'fall')\n",
    "    fall_images = [os.path.join(fall_dir, img) for img in os.listdir(fall_dir)] if os.path.exists(fall_dir) else []\n",
    "\n",
    "    if fall_images:\n",
    "        train, val = train_test_split(fall_images, test_size=0.4, random_state=42)\n",
    "        train_fall += train\n",
    "        validation_fall += val\n",
    "\n",
    "    # Non-fall\n",
    "    non_fall_dir = os.path.join(processed_dataset_dir, subject, 'non_fall')\n",
    "    non_fall_images = [os.path.join(non_fall_dir, img) for img in os.listdir(non_fall_dir)] if os.path.exists(non_fall_dir) else []\n",
    "\n",
    "    if non_fall_images:\n",
    "        train, val = train_test_split(non_fall_images, test_size=0.4, random_state=42)\n",
    "        train_non_fall += train\n",
    "        validation_non_fall += val\n",
    "\n",
    "# Step 4: Memindahkan file ke folder train dan validation\n",
    "def move_files(file_list, target_dir):\n",
    "    for file_path in file_list:\n",
    "        filename = os.path.basename(file_path)\n",
    "        target_path = os.path.join(target_dir, filename)\n",
    "\n",
    "        if not os.path.exists(target_path):\n",
    "            os.rename(file_path, target_path)\n",
    "\n",
    "# Memindahkan data fall\n",
    "move_files(train_fall, train_fall_dir)\n",
    "move_files(validation_fall, validation_fall_dir)\n",
    "\n",
    "# Memindahkan data non-fall\n",
    "move_files(train_non_fall, train_non_fall_dir)\n",
    "move_files(validation_non_fall, validation_non_fall_dir)\n",
    "\n",
    "print(\"Proses selesai!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # membagi data train dan data validation dengan perbandingan 60:40 \n",
    "\n",
    "# subjects = [f for f in os.listdir('./processed') if os.path.isdir(os.path.join('./processed', f))]\n",
    "# train_fall = []\n",
    "# validation_fall = []\n",
    "\n",
    "# for subject in subjects:\n",
    "#     fall_images = os.listdir(os.path.join('./processed', subject, 'fall'))\n",
    "#     train, val = train_test_split(fall_images, test_size=0.4, random_state=None)\n",
    "#     train_fall += [os.path.join(subject, 'fall', img) for img in train]\n",
    "#     validation_fall += [os.path.join(subject, 'fall', img) for img in val]\n",
    "\n",
    "# # train test split jika direktori belum ada isinya\n",
    "# if len(os.listdir('./train/fall')) == 0 and len(os.listdir('./train/non_fall')) == 0 :\n",
    "#     # fall\n",
    "#     train_fall = os.listdir('./processed/fall')\n",
    "#     validation_fall = os.listdir('./processed/fall')\n",
    "#     train_fall, validation_fall = train_test_split(train_fall, test_size = 0.4, random_state=None)\n",
    "\n",
    "#     # non_fall\n",
    "#     train_non_fall = os.listdir('./processed/non_fall')\n",
    "#     validation_non_fall = os.listdir('./processed/non_fall')\n",
    "#     train_non_fall, validation_non_fall = train_test_split(train_non_fall, test_size = 0.4, random_state=None)\n",
    "\n",
    "#     # Memindahkan file gambar ke direktori data training (train)\n",
    "#     for image in train_fall:\n",
    "#         shutil.move(os.path.join('./processed/fall', image), os.path.join(train_fall_dir, image))\n",
    "#     for image in train_non_fall:\n",
    "#         shutil.move(os.path.join('./processed/non_fall', image), os.path.join(train_non_fall_dir, image))\n",
    "\n",
    "#     # Memindahkan file gambar ke direktori data validasi (validation)\n",
    "#     for image in validation_fall:\n",
    "#         shutil.move(os.path.join('./processed/fall', image), os.path.join(validation_fall_dir, image))\n",
    "#     for image in validation_non_fall:\n",
    "#         shutil.move(os.path.join('./processed/non_fall', image), os.path.join(validation_non_fall_dir, image))\n",
    "# else:\n",
    "#     print('Direktori sudah ada isinya')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset berhasil diproses dan dipisahkan ke dalam direktori training dan validasi.\n"
     ]
    }
   ],
   "source": [
    "# # memindahkan file ke direktori training dan validasi\n",
    "# for subject, category, image in train_fall:\n",
    "#     src = os.path.join(processed_dataset_dir, subject, category, image)\n",
    "#     dst = os.path.join(train_fall_dir, image)\n",
    "#     shutil.move(src, dst)\n",
    "\n",
    "# for subject, category, image in validation_fall:\n",
    "#     src = os.path.join(processed_dataset_dir, subject, category, image)\n",
    "#     dst = os.path.join(validation_fall_dir, image)\n",
    "#     shutil.move(src, dst)\n",
    "\n",
    "# for subject, category, image in train_non_fall:\n",
    "#     src = os.path.join(processed_dataset_dir, subject, category, image)\n",
    "#     dst = os.path.join(train_non_fall_dir, image)\n",
    "#     shutil.move(src, dst)\n",
    "\n",
    "# for subject, category, image in validation_non_fall:\n",
    "#     src = os.path.join(processed_dataset_dir, subject, category, image)\n",
    "#     dst = os.path.join(validation_non_fall_dir, image)\n",
    "#     shutil.move(src, dst)\n",
    "\n",
    "# print(\"Dataset berhasil diproses dan dipisahkan ke dalam direktori training dan validasi.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Path dataset hasil pemrosesan dan direktori untuk train/validation\n",
    "processed_dir = './processed'\n",
    "train_dir = './processed_split/train'\n",
    "validation_dir = './processed_split/val'\n",
    "\n",
    "# Subdirektori kategori untuk training dan validation\n",
    "train_fall_dir = os.path.join(train_dir, 'fall')\n",
    "train_non_fall_dir = os.path.join(train_dir, 'non_fall')\n",
    "validation_fall_dir = os.path.join(validation_dir, 'fall')\n",
    "validation_non_fall_dir = os.path.join(validation_dir, 'non_fall')\n",
    "\n",
    "# Membuat semua direktori jika belum ada\n",
    "for directory in [train_fall_dir, train_non_fall_dir, validation_fall_dir, validation_non_fall_dir]:\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "# Mengambil daftar subject\n",
    "subjects = [f for f in os.listdir(processed_dir) if os.path.isdir(os.path.join(processed_dir, f))]\n",
    "\n",
    "# Membagi data train dan validation\n",
    "train_fall = []\n",
    "validation_fall = []\n",
    "train_non_fall = []\n",
    "validation_non_fall = []\n",
    "\n",
    "for subject in subjects:\n",
    "    # Fall\n",
    "    fall_dir = os.path.join(processed_dir, subject, 'fall')\n",
    "    if os.path.exists(fall_dir):\n",
    "        fall_images = os.listdir(fall_dir)\n",
    "        train, val = train_test_split(fall_images, test_size=0.4, random_state=42)\n",
    "        train_fall += [(os.path.join(fall_dir, img), os.path.join(train_fall_dir, f\"{subject}_{img}\")) for img in train]\n",
    "        validation_fall += [(os.path.join(fall_dir, img), os.path.join(validation_fall_dir, f\"{subject}_{img}\")) for img in val]\n",
    "\n",
    "    # Non-fall\n",
    "    non_fall_dir = os.path.join(processed_dir, subject, 'non_fall')\n",
    "    if os.path.exists(non_fall_dir):\n",
    "        non_fall_images = os.listdir(non_fall_dir)\n",
    "        train, val = train_test_split(non_fall_images, test_size=0.4, random_state=42)\n",
    "        train_non_fall += [(os.path.join(non_fall_dir, img), os.path.join(train_non_fall_dir, f\"{subject}_{img}\")) for img in train]\n",
    "        validation_non_fall += [(os.path.join(non_fall_dir, img), os.path.join(validation_non_fall_dir, f\"{subject}_{img}\")) for img in val]\n",
    "\n",
    "# Memindahkan file ke direktori train dan validation\n",
    "def move_files(file_list):\n",
    "    for src, dest in file_list:\n",
    "        if not os.path.exists(dest):  # Pastikan tidak overwrite\n",
    "            shutil.move(src, dest)\n",
    "\n",
    "# Memindahkan data fall\n",
    "move_files(train_fall)\n",
    "move_files(validation_fall)\n",
    "\n",
    "# Memindahkan data non-fall\n",
    "move_files(train_non_fall)\n",
    "move_files(validation_non_fall)\n",
    "\n",
    "print(\"Proses pembagian train dan validation selesai!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Path direktori\n",
    "processed_dataset_dir = './processed'\n",
    "processed_split_dir = './processed_split'\n",
    "train_dir = os.path.join(processed_split_dir, 'train')\n",
    "validation_dir = os.path.join(processed_split_dir, 'val')\n",
    "\n",
    "# Subdirektori kategori\n",
    "train_fall_dir = os.path.join(train_dir, 'fall')\n",
    "train_non_fall_dir = os.path.join(train_dir, 'non_fall')\n",
    "validation_fall_dir = os.path.join(validation_dir, 'fall')\n",
    "validation_non_fall_dir = os.path.join(validation_dir, 'non_fall')\n",
    "\n",
    "# Membuat semua direktori jika belum ada\n",
    "for directory in [train_fall_dir, train_non_fall_dir, validation_fall_dir, validation_non_fall_dir]:\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "# Mengambil daftar subject\n",
    "subjects = [f for f in os.listdir(processed_dataset_dir) if os.path.isdir(os.path.join(processed_dataset_dir, f))]\n",
    "\n",
    "# Membagi dataset menjadi train dan validation\n",
    "train_fall = []\n",
    "validation_fall = []\n",
    "train_non_fall = []\n",
    "validation_non_fall = []\n",
    "\n",
    "for subject in subjects:\n",
    "    # Fall\n",
    "    fall_dir = os.path.join(processed_dataset_dir, subject, 'fall')\n",
    "    if os.path.exists(fall_dir):\n",
    "        fall_images = os.listdir(fall_dir)\n",
    "        train, val = train_test_split(fall_images, test_size=0.4, random_state=42)\n",
    "        train_fall += [(subject, 'fall', img) for img in train]\n",
    "        validation_fall += [(subject, 'fall', img) for img in val]\n",
    "\n",
    "    # Non-fall\n",
    "    non_fall_dir = os.path.join(processed_dataset_dir, subject, 'non_fall')\n",
    "    if os.path.exists(non_fall_dir):\n",
    "        non_fall_images = os.listdir(non_fall_dir)\n",
    "        train, val = train_test_split(non_fall_images, test_size=0.4, random_state=42)\n",
    "        train_non_fall += [(subject, 'non_fall', img) for img in train]\n",
    "        validation_non_fall += [(subject, 'non_fall', img) for img in val]\n",
    "\n",
    "# Memindahkan file ke direktori training dan validasi\n",
    "def move_files(file_list, target_dir):\n",
    "    for subject, category, image in file_list:\n",
    "        src = os.path.join(processed_dataset_dir, subject, category, image)\n",
    "        dst = os.path.join(target_dir, f\"{subject}_{image}\")  # Tambahkan prefix subject untuk menghindari konflik nama\n",
    "        if not os.path.exists(dst):  # Hindari overwrite\n",
    "            shutil.move(src, dst)\n",
    "\n",
    "# Memindahkan file Fall\n",
    "move_files(train_fall, train_fall_dir)\n",
    "move_files(validation_fall, validation_fall_dir)\n",
    "\n",
    "# Memindahkan file Non-Fall\n",
    "move_files(train_non_fall, train_non_fall_dir)\n",
    "move_files(validation_non_fall, validation_non_fall_dir)\n",
    "\n",
    "print(\"Dataset berhasil diproses dan dipisahkan ke dalam direktori training dan validasi.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['non_fall', 'fall']"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# menampilkan nama kelas yang ada pada train\n",
    "os.listdir('./processed/train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['non_fall', 'fall']"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# menampilkan nama kelas yang ada pada train\n",
    "os.listdir('./processed/val')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img\n",
    "from sklearn.model_selection import train_test_split\n",
    "from rembg import remove\n",
    "from PIL import Image\n",
    "\n",
    "# 1. Fungsi Preprocessing Gambar\n",
    "def preprocess_img(img):\n",
    "    img = cv2.GaussianBlur(img, (5, 5), 0)  # Menghaluskan gambar\n",
    "    img = tf.image.adjust_contrast(img, 2)  # Menyesuaikan kontras gambar\n",
    "    return img\n",
    "\n",
    "# 2. Fungsi Penghapusan Background\n",
    "def remove_background(input_path, output_path):\n",
    "    try:\n",
    "        input_image = Image.open(input_path)\n",
    "        output_image = remove(input_image)\n",
    "        output_image.save(output_path)\n",
    "    except Exception as e:\n",
    "        with open(\"error_log.txt\", \"a\") as log_file:\n",
    "            log_file.write(f\"Error processing {input_path}: {e}\\n\")\n",
    "\n",
    "def remove_background_serial(input_dir, output_dir):\n",
    "    if not os.path.exists(output_dir):\n",
    "        os.makedirs(output_dir)\n",
    "\n",
    "    for filename in os.listdir(input_dir):\n",
    "        input_path = os.path.join(input_dir, filename)\n",
    "        output_path = os.path.join(output_dir, filename)\n",
    "        remove_background(input_path, output_path)\n",
    "\n",
    "# 3. Path Dataset\n",
    "original_dataset_dir = './train'\n",
    "processed_dataset_dir = './processed'\n",
    "categories = ['fall', 'non_fall']\n",
    "\n",
    "# Memproses dataset untuk setiap subject dan kategori\n",
    "subjects = [f for f in os.listdir(original_dataset_dir) if os.path.isdir(os.path.join(original_dataset_dir, f))]\n",
    "for subject in subjects:\n",
    "    for category in categories:\n",
    "        input_dir = os.path.join(original_dataset_dir, subject, category)\n",
    "        output_dir = os.path.join(processed_dataset_dir, subject, category)\n",
    "        if os.path.exists(input_dir):\n",
    "            remove_background_serial(input_dir, output_dir)\n",
    "\n",
    "# 4. Membagi Data Train dan Validation\n",
    "base_dir = './processed_split'\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "validation_dir = os.path.join(base_dir, 'val')\n",
    "train_fall_dir = os.path.join(train_dir, 'fall')\n",
    "train_non_fall_dir = os.path.join(train_dir, 'non_fall')\n",
    "validation_fall_dir = os.path.join(validation_dir, 'fall')\n",
    "validation_non_fall_dir = os.path.join(validation_dir, 'non_fall')\n",
    "\n",
    "for directory in [train_dir, validation_dir, train_fall_dir, train_non_fall_dir, validation_fall_dir, validation_non_fall_dir]:\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "train_fall = []\n",
    "validation_fall = []\n",
    "train_non_fall = []\n",
    "validation_non_fall = []\n",
    "\n",
    "for subject in subjects:\n",
    "    fall_dir = os.path.join(processed_dataset_dir, subject, 'fall')\n",
    "    fall_images = os.listdir(fall_dir) if os.path.exists(fall_dir) else []\n",
    "    if len(fall_images) > 0:\n",
    "        train, val = train_test_split(fall_images, test_size=0.4, random_state=42)\n",
    "        train_fall += [(subject, 'fall', img) for img in train]\n",
    "        validation_fall += [(subject, 'fall', img) for img in val]\n",
    "\n",
    "    non_fall_dir = os.path.join(processed_dataset_dir, subject, 'non_fall')\n",
    "    non_fall_images = os.listdir(non_fall_dir) if os.path.exists(non_fall_dir) else []\n",
    "    if len(non_fall_images) > 0:\n",
    "        train, val = train_test_split(non_fall_images, test_size=0.4, random_state=42)\n",
    "        train_non_fall += [(subject, 'non_fall', img) for img in train]\n",
    "        validation_non_fall += [(subject, 'non_fall', img) for img in val]\n",
    "\n",
    "for subject, category, image in train_fall:\n",
    "    src = os.path.join(processed_dataset_dir, subject, category, image)\n",
    "    dst = os.path.join(train_fall_dir, f\"{subject}_{image}\")\n",
    "    shutil.move(src, dst)\n",
    "\n",
    "for subject, category, image in validation_fall:\n",
    "    src = os.path.join(processed_dataset_dir, subject, category, image)\n",
    "    dst = os.path.join(validation_fall_dir, f\"{subject}_{image}\")\n",
    "    shutil.move(src, dst)\n",
    "\n",
    "for subject, category, image in train_non_fall:\n",
    "    src = os.path.join(processed_dataset_dir, subject, category, image)\n",
    "    dst = os.path.join(train_non_fall_dir, f\"{subject}_{image}\")\n",
    "    shutil.move(src, dst)\n",
    "\n",
    "for subject, category, image in validation_non_fall:\n",
    "    src = os.path.join(processed_dataset_dir, subject, category, image)\n",
    "    dst = os.path.join(validation_non_fall_dir, f\"{subject}_{image}\")\n",
    "    shutil.move(src, dst)\n",
    "\n",
    "# 5. ImageDataGenerator untuk Training dan Validation\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=40,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    shear_range=0.2,\n",
    "    fill_mode='nearest',\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    preprocessing_function=preprocess_img,\n",
    "    brightness_range=[0.5, 1.5],\n",
    ")\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "# 6. Load dan Augmentasi Gambar untuk Visualisasi\n",
    "img_path = os.path.join(train_fall_dir, os.listdir(train_fall_dir)[0])  # Ambil gambar pertama dari train/fall\n",
    "img = tf.keras.preprocessing.image.load_img(img_path)\n",
    "x = tf.keras.preprocessing.image.img_to_array(img)\n",
    "x = x.reshape((1,) + x.shape)\n",
    "\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.suptitle('Augmented Images')\n",
    "\n",
    "i = 0\n",
    "for batch in train_datagen.flow(x, batch_size=1):\n",
    "    plt.subplot(3, 3, i + 1)\n",
    "    plt.imshow(array_to_img(batch[0]))\n",
    "    i += 1\n",
    "    if i >= 6:\n",
    "        break\n",
    "\n",
    "plt.show()\n",
    "\n",
    "print(\"Proses selesai. Dataset telah diproses, dibagi menjadi train dan validation, dan augmentasi telah selesai.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Menggunakan objek ImageDataGenerator sebelumnya untuk mempersiapkan data latih dan validasi\n",
    "\n",
    "# Data training\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        train_dir,              # direktori data latih\n",
    "        target_size=(150, 150), # mengubah resolusi seluruh gambar menjadi 150x150 piksel\n",
    "        batch_size=32,          # jumlah sampel yang diproses dalam satu iterasi\n",
    "        class_mode='binary'     # karena ini adalah klasifikasi 2 kelas (fall dan non-fall), gunakan 'binary'\n",
    ")\n",
    "\n",
    "# Data validation\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_dir,         # direktori data validasi\n",
    "        target_size=(150, 150), # mengubah resolusi seluruh gambar menjadi 150x150 piksel\n",
    "        batch_size=32,          # jumlah sampel yang diproses dalam satu iterasi\n",
    "        class_mode='binary'     # karena ini adalah klasifikasi 2 kelas (fall dan non-fall), gunakan 'binary'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers, models\n",
    "\n",
    "# Arsitektur model sederhana\n",
    "model = models.Sequential([\n",
    "    layers.Conv2D(32, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(64, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Conv2D(128, (3, 3), activation='relu'),\n",
    "    layers.MaxPooling2D((2, 2)),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')  # Aktivasi sigmoid untuk klasifikasi biner\n",
    "])\n",
    "\n",
    "# Kompilasi model\n",
    "model.compile(\n",
    "    loss='binary_crossentropy', # Loss function untuk klasifikasi biner\n",
    "    optimizer='adam',           # Optimizer\n",
    "    metrics=['accuracy']        # Metode evaluasi\n",
    ")\n",
    "\n",
    "# Melatih model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=len(train_generator),\n",
    "    epochs=20,                   # Jumlah epoch\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=len(validation_generator)\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
